{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81059b0c-b61b-471b-b525-986e102ce497",
   "metadata": {},
   "source": [
    "# SIGNA Chapter 2 Implementation\n",
    "This notebook basically goes through backpropagation 101. We will try to optimize the same architecture we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbea2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m \u001b[38;5;66;03m# Gives us relu()\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SGD \u001b[38;5;66;03m# Note here we'll use SGD like traditional GD\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mL\u001b[39;00m \u001b[38;5;66;03m## New thing!\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader \u001b[38;5;66;03m## We'll store our data in DataLoaders\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn ## Gives us nn.Module()\n",
    "import torch.nn.functional as F # Gives us relu()\n",
    "from torch.optim import SGD # Note here we'll use SGD like traditional GD\n",
    "\n",
    "import lightning as L ## New thing!\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d1c69",
   "metadata": {},
   "source": [
    "## Desigining Our Neural Net\n",
    "The architecture should do the following things.\n",
    "1. Start with random initial weights and biases.\n",
    "2. Generate an output \"propagating forward\" (i.e. an epoch), and calculate the gradient of our Empirical Loss (MSE in our case) w.r.t. a weight/bias.\n",
    "3. Use this gradient and the learning rate parameter to calculate a step size via SGD.\n",
    "4. Use the step size to calculate new weights, which will \"propagate backward\" (hence why we call it backpropagation).\n",
    "5. Repeat this process until the weights and biases are optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a581082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        # Kinda like a constructor where we initialize varibles\n",
    "        # Start by calling the superclass initializer\n",
    "        super().__init__()\n",
    "\n",
    "        # Intialize random weights and biases\n",
    "        # Note, that here I'm just following the book, and bias usually starts at zero\n",
    "        self.w1 = torch.tensor(0.06)\n",
    "        self.b1 = torch.tensor(0)\n",
    "\n",
    "        self.w2 = torch.tensor(3.49)\n",
    "        self.b2 = torch.tensor(0)\n",
    "\n",
    "        self.w3 = torch.tensor(-4.11)\n",
    "        self.w4 = torch.tensor(2.74)\n",
    "\n",
    "        # Define the empirical loss function of choice\n",
    "        self.loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        # This method runs inputs through the network to make a prediction\n",
    "        # This code is identical to the previous notebook\n",
    "        top_x_axis_values = input_values * self.w1 + self.b1\n",
    "        top_y_axis_values = F.relu(top_x_axis_values)\n",
    "\n",
    "        bottom_x_axis_values = input_values * self.w2 + self.b2\n",
    "        bottom_y_axis_values = F.relu(bottom_x_axis_values)\n",
    "\n",
    "        output_values = top_y_axis_values * self.w3 + bottom_y_axis_values * self.w4\n",
    "        return output_values\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # New method which deals with the learning rate and optimization algorithm\n",
    "        # Perform gradient descent on the params, with learning rate 0.01\n",
    "        return SGD(self.parameters(), lr=0.01)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # This method basically runs one epoch\n",
    "        # Pass training data to forward() and calculate residuals\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32394a5e",
   "metadata": {},
   "source": [
    "## Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7184b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some inputs and labels (i.e. true outcomes)\n",
    "training_inputs = torch.tensor([0.0, 0.5, 1.0])\n",
    "training_labels = torch.tensor([0.0, 1.0, 0.0])\n",
    "\n",
    "# Package everything up into a DataLoader\n",
    "training_dataset = TensorDataset(training_inputs, training_labels)\n",
    "dataloader = DataLoader(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464be65",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = myNN()\n",
    "\n",
    "# Create a trainer object\n",
    "trainer = L.Trainer(max_epochs=500, # essentially how many times we call training_step()\n",
    "                    logger=False,\n",
    "                    enable_checkpointing=False,\n",
    "                    enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d260b5a",
   "metadata": {},
   "source": [
    "## Plug and Chug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the different doses we want to run through the neural network.\n",
    "# torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
    "# So this is 0.0, 0.1, ..., 1.0\n",
    "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
    "\n",
    "# Optionally, we can round the output values to match the scale we want\n",
    "torch.round(model(input_doses), decimals=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
