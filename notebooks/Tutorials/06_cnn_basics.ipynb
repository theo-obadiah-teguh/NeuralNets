{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64097a0d-b987-435e-bda3-79146f23b055",
   "metadata": {},
   "source": [
    "# SIGNA Chapter 6 Implementation\n",
    "In this notebook, we explore the basic architecture of a CNN. Our design will include the following.\n",
    "1. Kernel/Filter + Bias, resulting in a Feature Map, which will be passed onto an Activation Function with ReLU.\n",
    "3. Pooling Layer with max pooling.\n",
    "4. Finally we take the output and use a Feed-Forward Network (with 4 inputs, a ReLU layer, and 2 outputs).\n",
    "5. We can optionally use SoftMax or ArgMax to simplify the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2991b5e5-2dfd-47e3-a4ef-6629521ac180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234397b-8515-41b7-8274-a5e863fbc8d9",
   "metadata": {},
   "source": [
    "## Designing Our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d81ba9b-1b7f-4763-bd26-a60082adf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # For Part 1\n",
    "        # Note: Since our input is black and white, we have one input channel\n",
    "        # If we had three channels, e.g. RGB, then in_channels=3\n",
    "        # Similar to the output_channel, we can decide how many channels based on our needs\n",
    "        # If we wanted a non-square kernel, use tuples, e.g. (2,3)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "        # For Part 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # For Part 3\n",
    "        self.input_to_hidden = nn.Linear(in_features=4, out_features=1)\n",
    "        self.hidden_to_output = nn.Linear(in_features=1, out_features=2)\n",
    "\n",
    "        # Determine the loss function\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Part 1\n",
    "        output_data = self.conv(input_data)\n",
    "        output_data = F.relu(output_data)\n",
    "\n",
    "        # Part 2\n",
    "        output_data = self.pool(output_data)\n",
    "\n",
    "        # Now, at this point we have a square matrix of values.\n",
    "        # Use torch.flatten() to turn the matrix into a vector.\n",
    "        output_data = torch.flatten(output_data, 1) # flatten all dimensions except batch \n",
    "\n",
    "        # Part 3\n",
    "        output_data = self.input_to_hidden(output_data)\n",
    "        output_data = F.relu(output_data)\n",
    "        output_data = self.hidden_to_output(output_data)\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        # Output training step loss\n",
    "        self.log(\"CE Loss: \", loss) \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abb921-bbef-45ae-a14f-7b08fc38b054",
   "metadata": {},
   "source": [
    "## Prepare Basic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96d7a0e-f5a9-4f74-b1a7-af5cf79156c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0\n",
      "tensor([[[0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.]]])\n",
      "tensor([[1., 0.]])\n",
      "\n",
      "batch_num: 1\n",
      "tensor([[[1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[0., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a 6x6 matrix of numbers where 0 represents white and 1 represents black.\n",
    "o_image = [[0, 0, 1, 1, 0, 0],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [1, 0, 0, 0, 0, 1],\n",
    "           [1, 0, 0, 0, 0, 1],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [0, 0, 1, 1, 0, 0]]\n",
    "\n",
    "x_image = [[1, 0, 0, 0, 0, 1],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [0, 0, 1, 1, 0, 0],\n",
    "           [0, 0, 1, 1, 0, 0],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [1, 0, 0, 0, 0, 1]]\n",
    "\n",
    "# Convert the images into tensors...\n",
    "input_images = torch.tensor([o_image, x_image]).type(torch.float32)\n",
    "\n",
    "# Create the labels for the input images\n",
    "input_labels = torch.tensor([[1.0, 0.0], [0.0, 1.0]]).type(torch.float32)\n",
    "\n",
    "# Now combine input_images and input_labels into a TensorDataset and create a DataLoader\n",
    "dataset = TensorDataset(input_images, input_labels) \n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "# Just show the dataloader contents\n",
    "for batch_num, (images, labels) in enumerate(dataloader): \n",
    "    print(\"batch_num:\", batch_num)\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2efcd-4884-4249-9154-59778b3b1d28",
   "metadata": {},
   "source": [
    "## Training Our CNN: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b20ea80-33f8-4c3a-aae0-b48778b58001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | conv             | Conv2d           | 10     | train\n",
      "1 | pool             | MaxPool2d        | 0      | train\n",
      "2 | input_to_hidden  | Linear           | 5      | train\n",
      "3 | hidden_to_output | Linear           | 4      | train\n",
      "4 | loss             | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 269.76it/s, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 189.77it/s, v_num=18]\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = myCNN()\n",
    "\n",
    "# Create a trainer for the model\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6da3af-6fc9-4155-844f-9f8439aa7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.4500, 0.5500]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "\n",
      "\n",
      "predicted_label: tensor([[0.3600, 0.6400]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    # First, run the image through the model to make a prediction\n",
    "    prediction = model(image)\n",
    "    \n",
    "    # Now make the prediction easy to read and interpret by\n",
    "    # running it through torch.softmax() and torch.round()\n",
    "    # dim=0 applies softmax to rows, dim=1 applies soft to columns\n",
    "    predicted_label = torch.round(torch.softmax(prediction, dim=1), decimals=2) \n",
    "    \n",
    "    # Now print out the the predicted label and the original label\n",
    "    # so we see how well our CNN performed.\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2deae3-263c-489e-bb91-503cc829cae4",
   "metadata": {},
   "source": [
    "## Training Our CNN: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc06a12-849b-465a-a0e5-e5606f8f79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /notebooks/lightning_logs/version_18/checkpoints/epoch=99-step=200.ckpt\n",
      "/usr/local/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:366: The dirpath has changed from '/notebooks/lightning_logs/version_18/checkpoints' to '/notebooks/lightning_logs/version_19/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | conv             | Conv2d           | 10     | train\n",
      "1 | pool             | MaxPool2d        | 0      | train\n",
      "2 | input_to_hidden  | Linear           | 5      | train\n",
      "3 | hidden_to_output | Linear           | 4      | train\n",
      "4 | loss             | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /notebooks/lightning_logs/version_18/checkpoints/epoch=99-step=200.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 260.09it/s, v_num=19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=700` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 175.59it/s, v_num=19]\n"
     ]
    }
   ],
   "source": [
    "# Continue training where we left off\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path # By default, \"best\" = \"most recent\"\n",
    "\n",
    "# Add another 600 epochs\n",
    "trainer = L.Trainer(max_epochs=700)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874579aa-8913-484b-82fd-dc8a0c739ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.9900, 0.0100]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "\n",
      "\n",
      "predicted_label: tensor([[0.2300, 0.7700]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance.. again\n",
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    # First, run the image through the model to make a prediction\n",
    "    prediction = model(image)\n",
    "    \n",
    "    # Now make the prediction easy to read and interpret by\n",
    "    # running it through torch.softmax() and torch.round()\n",
    "    # dim=0 applies softmax to rows, dim=1 applies soft to columns\n",
    "    predicted_label = torch.round(torch.softmax(prediction, dim=1), decimals=2) \n",
    "    \n",
    "    # Now print out the the predicted label and the original label\n",
    "    # so we see how well our CNN performed.\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5413c9-e60a-4431-b668-1427d6403dec",
   "metadata": {},
   "source": [
    "## Using Some New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84458cfc-a4af-4c54-b945-bed3ccbef03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4000, 0.6000],\n",
       "        [0.2300, 0.7700]], grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_o_image = [[0, 1, 1, 0, 0, 0],\n",
    "                   [1, 0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1, 0],\n",
    "                   [1, 0, 0, 1, 0, 0],\n",
    "                   [0, 1, 1, 0, 0, 0]]\n",
    "\n",
    "shifted_x_image = [[0, 1, 0, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0, 1],\n",
    "                   [0, 0, 0, 1, 1, 0],\n",
    "                   [0, 0, 0, 1, 1, 0],\n",
    "                   [0, 0, 1, 0, 0, 1],\n",
    "                   [0, 1, 0, 0, 0, 0]]\n",
    "\n",
    "# Create the batch tensor\n",
    "test_images = torch.tensor([shifted_o_image, shifted_x_image]).type(torch.float32)\n",
    "\n",
    "# CNNs expect input in the format [batch, channels, height, width].\n",
    "# The 1 in unsqueeze(1) specifies where to insert the new dimension - at index position 1.\n",
    "# Add the channel dimension - this changes shape from [2, 6, 6, missing_width] to [2, 1, 6, 6]\n",
    "test_images = test_images.unsqueeze(1)  # or test_images[:, None, :, :]\n",
    "\n",
    "# Now run prediction\n",
    "predictions = model(test_images)\n",
    "\n",
    "# And generate labels\n",
    "predicted_labels = torch.round(torch.softmax(predictions, dim=1), decimals=2) ## dim=0 applies argmax to rows, dim=1 applies argmax to colum\n",
    "predicted_labels\n",
    "\n",
    "# Note that the correct labels should be\n",
    "# [1, 0] for shifted O\n",
    "# [0, 1] for shifted X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
